{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "#Neural network class def:\n",
    "\n",
    "class neuralNetwork:\n",
    "    \n",
    "    #initialize the nn\n",
    "    def __init__(self,inputnodes,hiddennodes,outputnodes,learningrate):\n",
    "        #set number of nodes in input, hidden,outpou layers\n",
    "        self.inodes=inputnodes\n",
    "        self.hnodes=hiddennodes\n",
    "        self.onodes=outputnodes\n",
    "        \n",
    "        #set learning rate\n",
    "        self.lr=learningrate\n",
    "        \n",
    "        #link weight matrices with wih and who\n",
    "        #weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        #w11 w21 w12 w22 etc.\n",
    "        #self.wih=(numpy.random.rand(self.hnodes,self.inodes)-0.5)\n",
    "        #self.who=(numpy.random.rand(self.onodes,self.hnodes)-0.5)\n",
    "        self.wih=numpy.random.normal(0.0,pow(self.hnodes,-0.5),(self.hnodes,self.inodes))\n",
    "        self.who=numpy.random.normal(0.0,pow(self.onodes,-0.5),(self.onodes,self.hnodes))\n",
    "        \n",
    "        #Activation function is the sigmoid function\n",
    "        self.activation_function=lambda x: scipy.special.expit(x)\n",
    "        self.inverse_activation_function = lambda x: scipy.special.logit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    #train the neural net\n",
    "    def train(self, inputs_list, target_lists):\n",
    "        \n",
    "        #convert input list to 2d array\n",
    "        inputs=numpy.array(inputs_list,ndmin=2).T\n",
    "        targets=numpy.array(target_lists,ndmin=2).T\n",
    "        \n",
    "        hidden_inputs=numpy.dot(self.wih,inputs)\n",
    "        hidden_outputs=self.activation_function(hidden_inputs)\n",
    "        \n",
    "        final_inputs=numpy.dot(self.who,hidden_outputs)\n",
    "        final_outputs=self.activation_function(final_inputs)\n",
    "        \n",
    "        #error\n",
    "        output_errors=targets-final_outputs\n",
    "        \n",
    "        #hidden errors is the output errors split by weights, recombined at hidden nodes\n",
    "        hidden_errors=numpy.dot(self.who.T,output_errors)\n",
    "        \n",
    "        #update the weights of links between the hidden and output layers\n",
    "        self.who += self.lr*numpy.dot((output_errors*final_outputs*(1.0-final_outputs)),numpy.transpose(hidden_outputs))\n",
    "        #update the weights of links between the input and hidden layers\n",
    "        self.wih += self.lr*numpy.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)),numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    #query the neural net\n",
    "    \n",
    "    def query(self,inputs_list):\n",
    "        #convert input list to 2d array\n",
    "        inputs=numpy.array(inputs_list,ndmin=2).T\n",
    "        \n",
    "        \n",
    "        #calculate the signals into hidden layer\n",
    "        hidden_inputs=numpy.dot(self.wih,inputs)\n",
    "        \n",
    "        #calculates the signals emerging from the hidden layer\n",
    "        hidden_outputs=self.activation_function(hidden_inputs)\n",
    "        \n",
    "        \n",
    "        #calculate the inputs of output layer\n",
    "        final_inputs=numpy.dot(self.who,hidden_outputs)\n",
    "        \n",
    "        #calculate the result of output layer\n",
    "        final_outputs=self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # backquery the neural network\n",
    "    # we'll use the same termnimology to each item, \n",
    "    # eg target are the values at the right of the network, albeit used as input\n",
    "    # eg hidden_output is the signal to the right of the middle nodes\n",
    "    def backquery(self, targets_list):\n",
    "        # transpose the targets list to a vertical array\n",
    "        final_outputs = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate the signal into the final output layer\n",
    "        final_inputs = self.inverse_activation_function(final_outputs)\n",
    "\n",
    "        # calculate the signal out of the hidden layer\n",
    "        hidden_outputs = numpy.dot(self.who.T, final_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        hidden_outputs -= numpy.min(hidden_outputs)\n",
    "        hidden_outputs /= numpy.max(hidden_outputs)\n",
    "        hidden_outputs *= 0.98\n",
    "        hidden_outputs += 0.01\n",
    "        \n",
    "        # calculate the signal into the hidden layer\n",
    "        hidden_inputs = self.inverse_activation_function(hidden_outputs)\n",
    "        \n",
    "        # calculate the signal out of the input layer\n",
    "        inputs = numpy.dot(self.wih.T, hidden_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        inputs -= numpy.min(inputs)\n",
    "        inputs /= numpy.max(inputs)\n",
    "        inputs *= 0.98\n",
    "        inputs += 0.01\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
